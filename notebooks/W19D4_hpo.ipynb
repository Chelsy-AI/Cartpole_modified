{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W19D4 ‚Äî Hyperparameter Optimization with Optuna\n",
    "\n",
    "**Team:** [Your Team Name]  \n",
    "**Runner:** [Name]  \n",
    "**Date:** [Date]\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. Define a search space for PPO hyperparameters\n",
    "2. Run an Optuna HPO sweep\n",
    "3. Export leaderboard and best config\n",
    "4. Select a ship candidate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (uncomment if running in Colab)\n",
    "# !pip install gymnasium stable-baselines3 optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your trial budget here. Start small in-class (8-12), expand for Sunday (25+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURATION ===\nN_TRIALS = 10          # In-class: 8-12, Sunday: 25+\nTIMESTEPS_PER_TRIAL = 20_000  # Shorter for HPO speed\nN_EVAL_EPISODES = 5    # Episodes for evaluation\nSEED = 42              # Base seed\n\n# Output paths\nRESULTS_DIR = \"../results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nprint(f\"Running {N_TRIALS} trials, {TIMESTEPS_PER_TRIAL:,} timesteps each\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Search Space\n",
    "\n",
    "The objective function defines:\n",
    "1. Which hyperparameters to tune (search space)\n",
    "2. How to train with those hyperparameters\n",
    "3. What metric to return (for Optuna to optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for PPO hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object that suggests hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean evaluation reward (higher is better)\n",
    "    \"\"\"\n",
    "    \n",
    "    # === SEARCH SPACE ===\n",
    "    # Optuna will try different values within these ranges\n",
    "    \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [16, 32, 64, 128, 256, 512])\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-8, 0.1, log=True)\n",
    "    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.4)\n",
    "    \n",
    "    # === CREATE ENVIRONMENT ===\n",
    "    env = make_vec_env(\"CartPole-v1\", n_envs=1, seed=SEED)\n",
    "    \n",
    "    # === CREATE MODEL ===\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        ent_coef=ent_coef,\n",
    "        clip_range=clip_range,\n",
    "        verbose=0,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    # === TRAIN ===\n",
    "    try:\n",
    "        model.learn(total_timesteps=TIMESTEPS_PER_TRIAL)\n",
    "    except Exception as e:\n",
    "        # If training fails, return a bad score\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return 0.0\n",
    "    \n",
    "    # === EVALUATE ===\n",
    "    eval_env = gym.make(\"CartPole-v1\")\n",
    "    mean_reward, _ = evaluate_policy(\n",
    "        model, \n",
    "        eval_env, \n",
    "        n_eval_episodes=N_EVAL_EPISODES,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "print(\"Objective function defined.\")\n",
    "print(\"Search space: learning_rate, n_steps, gamma, ent_coef, clip_range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run HPO Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study (maximize = higher reward is better)\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=SEED),\n",
    "    study_name=\"cartpole_ppo_hpo\"\n",
    ")\n",
    "\n",
    "print(f\"Starting HPO sweep with {N_TRIALS} trials...\")\n",
    "print(\"This may take a while. Progress will be shown below.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HPO SWEEP COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create leaderboard DataFrame\n",
    "trials_data = []\n",
    "for trial in study.trials:\n",
    "    row = {\n",
    "        \"trial\": trial.number,\n",
    "        \"mean_reward\": trial.value,\n",
    "        **trial.params\n",
    "    }\n",
    "    trials_data.append(row)\n",
    "\n",
    "leaderboard = pd.DataFrame(trials_data)\n",
    "leaderboard = leaderboard.sort_values(\"mean_reward\", ascending=False)\n",
    "\n",
    "# Save leaderboard\n",
    "leaderboard_path = os.path.join(RESULTS_DIR, \"hpo_leaderboard.csv\")\n",
    "leaderboard.to_csv(leaderboard_path, index=False)\n",
    "print(f\"‚úÖ Saved: {leaderboard_path}\")\n",
    "\n",
    "# Display top 10\n",
    "print(\"\\nTop 10 Trials:\")\n",
    "print(leaderboard.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best config\n",
    "best_config = {\n",
    "    \"trial_number\": study.best_trial.number,\n",
    "    \"mean_reward\": study.best_value,\n",
    "    \"params\": study.best_params\n",
    "}\n",
    "\n",
    "config_path = os.path.join(RESULTS_DIR, \"best_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "print(f\"‚úÖ Saved: {config_path}\")\n",
    "\n",
    "# Display best config\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Trial: {study.best_trial.number}\")\n",
    "print(f\"Mean Reward: {study.best_value:.2f}\")\n",
    "print(\"\\nHyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "try:\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "except:\n",
    "    print(\"Visualization not available (install plotly for interactive plots)\")\n",
    "    # Fallback: simple matplotlib plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot([t.number for t in study.trials], [t.value for t in study.trials], 'o-')\n",
    "    plt.xlabel(\"Trial\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.title(\"HPO Optimization History\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter importances\n",
    "try:\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.show()\n",
    "except:\n",
    "    print(\"Parameter importance plot not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary for Ship Candidate Selection\n",
    "\n",
    "Copy this to `docs/ship_candidate.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã COPY FOR docs/ship_candidate.md:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total trials: {len(study.trials)}\")\n",
    "print(f\"Best trial: #{study.best_trial.number}\")\n",
    "print(f\"Best mean reward: {study.best_value:.2f}\")\n",
    "print(\"\\nBest config JSON:\")\n",
    "print(json.dumps(study.best_params, indent=2))\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nLeaderboard saved to: {leaderboard_path}\")\n",
    "print(f\"Best config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### In-Class (W19D4)\n",
    "1. ‚úÖ Review leaderboard and select ship candidate\n",
    "2. ‚úÖ Complete `docs/ship_candidate.md`\n",
    "3. ‚úÖ Update `docs/runbook.md` with HPO run instructions\n",
    "\n",
    "### After Class (Due Sunday)\n",
    "1. ‚û°Ô∏è Runner: Expand sweep to 25+ trials (change N_TRIALS above)\n",
    "2. ‚û°Ô∏è Each fellow: Write individual Google Doc reflection\n",
    "3. ‚û°Ô∏è Update PR with final artifacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}